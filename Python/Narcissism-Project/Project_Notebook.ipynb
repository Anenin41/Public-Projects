{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "117b0c7c-7da6-4d03-bdf5-a0450728e6e5",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this jupyter notebook, all utility scripts for the Narcissism Project are showcased diligently. Most of the following code blocks have proper documentation, and require little to no modification in order to run on most computers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d8d2dc-62b7-4ae3-bbbf-c811b7a058ff",
   "metadata": {},
   "source": [
    "# analyst.py\n",
    "This script reads the corresponding file that is parsed as an argument, and calculate the occurences of \"I\", \"me\", \"my\", \"mine\", \"myself\", \"we\", \"us\", \"our\", \"ours\", \"ourselves\" keywords. In addition, a narcissism ratio is then produced by testing the singular to the plural pronounts. Finally, the results are stored in a dictionary data structure for later manipulation.\n",
    "\n",
    "### Package Imports\n",
    "- `re`: regular expression package for text cleaning.\n",
    "- `os`: package for path manipulation between folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1ca2962-22cf-4f4b-b717-57d57bd58164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyst.py\n",
    "\n",
    "# Packages\n",
    "import re\n",
    "import os\n",
    "\n",
    "def analyst(filename):\n",
    "    '''\n",
    "    Function which reads a .txt file (in this case a letter to shareholders) and counts the occurences \n",
    "    of singular and plural pronouns. It then calculates the normalized narcissism ration in the document\n",
    "    by testing singular / plural pronounts. Lastly, it stores the results in a dictionary data structure\n",
    "    for easier manipulation by other python scripts.\n",
    "\n",
    "    Input:\n",
    "        filename: relevant path of the .txt  file of a singular letter to shareholders.\n",
    "\n",
    "    Output:\n",
    "        dictionary data structure  with the following items:\n",
    "            \"filename\": the name of the file (e.g. letter_2023_387.txt)\n",
    "            \"I\": occurences of \"I\" in the document.\n",
    "            \"me\": occurences of \"me\" in the document.\n",
    "            \"my\": occurences of \"my\" in the document.\n",
    "            \"mine\": occurences of \"mine\" in the document.\n",
    "            \"we\": occurences of \"we\" in the document.\n",
    "            \"us\": occurences of \"us\" in the document.\n",
    "            \"our\": occurences of \"our\" in the document.\n",
    "            \"ours\": occurences of \"ours\" in the document.\n",
    "            \"ourselves\": occurences of \"ourselves\" in the document.\n",
    "            \"word_count\": number of words in the document.\n",
    "            \"narc_ratio\": the ratio of singular to plural pronouns.\n",
    "    '''\n",
    "    # Do main operations in a try-except-finally block, in order to avoid premature termination of \n",
    "    # the program due to unexpected errors.\n",
    "    try:\n",
    "        # Open desired file and read its contents\n",
    "        file = open(filename, \"r\", encoding=\"utf-8\", errors=\"ignore\")\n",
    "        text = file.read()\n",
    "\n",
    "        # Save the text in lowercase for easier manipulation\n",
    "        text_lower = text.lower()\n",
    "\n",
    "        # Count singular pronouns\n",
    "        i_count = len(re.findall(r\"\\bI\\b\", text))\n",
    "        me_count = len(re.findall(r\"\\bme\\b\", text_lower))\n",
    "        my_count = len(re.findall(r\"\\bmy\\b\", text_lower))\n",
    "        mine_count = len(re.findall(r\"\\bmine\\b\", text_lower))\n",
    "        myself_count = len(re.findall(r\"\\bmyself\\b\", text_lower))\n",
    "\n",
    "        # Count plural pronouns\n",
    "        we_count = len(re.findall(r\"\\bwe\\b\", text_lower))\n",
    "        us_count = len(re.findall(r\"\\bus\\b\", text_lower))\n",
    "        our_count = len(re.findall(r\"\\bour\\b\", text_lower))\n",
    "        ours_count = len(re.findall(r\"\\bours\\b\", text_lower))\n",
    "        ourselves_count = len(re.findall(r\"\\bourselves\\b\", text_lower))\n",
    "\n",
    "        # Total word count\n",
    "        total_word_count = len(re.findall(r\"\\b\\w+\\b\", text))\n",
    "\n",
    "        # Calculate summary of counters and narcissism ratio (also avoid division by zero)\n",
    "        nominator = i_count + me_count + my_count + mine_count + myself_count\n",
    "        denominator = we_count + us_count + our_count + ours_count + ourselves_count\n",
    "        if denominator == 0:\n",
    "            narc_ratio = 0\n",
    "        else:\n",
    "            narc_ratio = nominator / denominator\n",
    "        \n",
    "        # Store everything into a dictionary data structure, in order to make the conversion to an excel\n",
    "        # document later much easier.\n",
    "        results = {\n",
    "                \"file_name\": filename,\n",
    "                \"I\": i_count,\n",
    "                \"me\": me_count,\n",
    "                \"my\": my_count,\n",
    "                \"mine\": mine_count,\n",
    "                \"myself\": myself_count,\n",
    "                \"we\": we_count,\n",
    "                \"us\": us_count,\n",
    "                \"our\": our_count,\n",
    "                \"ours\": ours_count,\n",
    "                \"ourselves\": ourselves_count,\n",
    "                \"word_count\": total_word_count,\n",
    "                \"narc_ratio\": narc_ratio\n",
    "                }\n",
    "        # Return the dictionary in case it is needed later\n",
    "        return results\n",
    "    \n",
    "    # Handle exceptions of any type so the parent loop doesn't terminate prematurely.\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {filename}. Skipping.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "    finally:\n",
    "        # Close the file for memory efficiency\n",
    "        if file is not None:\n",
    "            file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5718c38-77eb-41f8-a6af-6768229e5fe2",
   "metadata": {},
   "source": [
    "# extractor.py\n",
    "This script extracts the required columns from the main Excel dataset. This is done in order to avoid manipulating the main file, and to save time by condensing the data into a smaller Excel spreadsheet.\n",
    "\n",
    "### Package Imports\n",
    "- `pandas`: package for reading Excel files and manipulating tabular data efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8533cba6-6e8b-48b8-90a0-58802a5d914a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extractor.py\n",
    "\n",
    "# Packages\n",
    "import pandas as pd\n",
    "\n",
    "def extractor(file_path):\n",
    "    '''\n",
    "    Function which extracts the required columns from the Master dataset, and\n",
    "    converts them to a Pandas DataFrame data structure for easier manipulation.\n",
    "\n",
    "    Input:\n",
    "        file_path: absolute path of the Master dataset for column extraction.\n",
    "\n",
    "    Output:\n",
    "        file: a new Excel spreadsheet with only the important data stored in it.\n",
    "    '''\n",
    "    # Store the columns (Excel-style letters) to be extracted in a list.\n",
    "    # Column A:     id_firm\n",
    "    # Column B:     firm_name\n",
    "    # Column M:     coder (also the name of the parent folder which stores the\n",
    "    #                      letters to shareholders)\n",
    "    # Column N:     year\n",
    "    # Column CN:    letter (filename of the letter to shareholders)\n",
    "    columns_to_extract = [\"id_firm\", \"firm_name\", \"coder\", \"year\", \"letter\"]\n",
    "    \n",
    "    # Read full dataset\n",
    "    df_full = pd.read_excel(file_path)\n",
    "\n",
    "    # Extract important columns and store them into a new DataFrame variable\n",
    "    df = df_full[columns_to_extract]\n",
    "\n",
    "    # Save important data to a new Excel spreadsheet\n",
    "    df.to_excel(\"extracted_data.xlsx\", index=False)\n",
    "    \n",
    "    # Print a flag on screen to showcase termination of the extraction operation.\n",
    "    print(\"Extraction complete...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71e14e8-b46c-467b-b0c9-a65e48cf355d",
   "metadata": {},
   "source": [
    "# config.json\n",
    "This utility file stores 5 Boolean (True or False) variables which make program manipulation much easier to perform. For example, once the required data has been extracted from the `extractor.py` function, then there is no need to extract them again."
   ]
  },
  {
   "cell_type": "raw",
   "id": "5df8358c-f167-4954-b386-6a076681f2ab",
   "metadata": {},
   "source": [
    "{\n",
    "    \"extract_data\": true,\n",
    "    \"analyse_data\": true,\n",
    "    \"global_search\": false,\n",
    "    \"local_search\": true,\n",
    "    \"merge_data\": true,\n",
    "}   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ed9106-2f13-428b-b637-c6ff475a29fc",
   "metadata": {},
   "source": [
    "# setup.py\n",
    "In order to correctly use the `config.json` file inside the python script, it is mandatory to load it and extract its data. This is done by the `setup.py` script, which loads the JSON file and returns its data in a Python format for the programs to use later.\n",
    "\n",
    "### Package Imports\n",
    "- `json`: library which provides code for JSON file manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac889dc6-6538-4db4-9dda-2688127f94da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup.py\n",
    "\n",
    "# Packages\n",
    "import json\n",
    "\n",
    "def load_config_flags(config_file=\"config.json\"):\n",
    "    '''\n",
    "    Python function which reads the config.json file, extracts the configuration\n",
    "    variables and stores them into Python format for later use.\n",
    "\n",
    "    Input:\n",
    "        confit_file: relevant path to the config.json file.\n",
    "\n",
    "    Output:\n",
    "        tuple: each item is a config variable.\n",
    "    '''\n",
    "\n",
    "    try:\n",
    "        with open(config_file, \"r\") as f:\n",
    "            config = json.load(f)\n",
    "        \n",
    "        # Extract the configuration variables.\n",
    "        extract_data = config.get(\"extract_data\", False)\n",
    "        analyse_data = config.get(\"analyse_data\", False)\n",
    "        global_search = config.get(\"global_search\", False)\n",
    "        local_search = config.get(\"local_search\", False)\n",
    "        merge_data = config.get(\"merge_data\", False)\n",
    "\n",
    "        return extract_data, analyse_data, global_search, local_search, merge_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occured: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0b89c7-6605-480a-9105-a89dd45a48d9",
   "metadata": {},
   "source": [
    "# merger.py\n",
    "This script extract important data from various Excel spreadsheets and merges them correctly and efficiently into a new Excel sheet. This is done with the singular purpose of making the STATA analysis of the new document more efficient, both for the computer and the user.\n",
    "\n",
    "### Package Imports\n",
    "- `pandas`: package for reading Excel files and manipulating tabular data efficiently.\n",
    "- `Path from pathlib`: a path manipulation library to efficiently shift between sub-folders.\n",
    "- `setup`: the config.json loader script from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc4967bd-6cfa-4914-8eb8-f69f92eee4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merger.py\n",
    "\n",
    "# Packages\n",
    "import pandas as pd             \n",
    "from pathlib import Path        \n",
    "from setup import *          \n",
    "\n",
    "def merger():\n",
    "    '''\n",
    "    Function which gathers information from different Excel spreadsheets and\n",
    "    merges them together into a master Excel document, for later use in STATA.\n",
    "\n",
    "    Output:\n",
    "        file: Excel spreadsheet with all of the important information for narcissism analysis in STATA, \n",
    "              including the name of the file, the occurences of the pronouns, and correct matching with \n",
    "              each individual document.\n",
    "    '''\n",
    "    # Get configuration variables (ignore config variables which are not of use here)\n",
    "    _, _, global_search, local_search, _ = load_config_flags()\n",
    "\n",
    "    # Load excel files\n",
    "    analysis_df = pd.read_excel(\"narcissism_analysis.xlsx\") # Narcissism data\n",
    "    master_df = pd.read_excel(\"extracted_data.xlsx\")        # Master Excel sheet\n",
    "\n",
    "    # Extract the narcissism information for each letter to shareholders. \n",
    "    if global_search:\n",
    "        # Extract narcissism data for each file \"Letters/filename_DATE_ID.txt\", \n",
    "        # and match the columns at the appropriate location in master document.\n",
    "        analysis_df[\"letter\"] = analysis_df[\"file_name\"].apply(lambda x: Path(x).name)\n",
    "    elif local_search:\n",
    "        # Extract narcissism data for each file \"Letters/STUDENT/filename_DATE_ID.txt\",\n",
    "        # and match the columns at the appropriate location in master document.\n",
    "        analysis_df[\"letter\"] = analysis_df[\"file_name\"].apply(\n",
    "                lambda x: str(x).split(\"/\")[-1].split(\"\\\\\")[-1]\n",
    "                )\n",
    "\n",
    "    # Merge the extracted information into appropriate columns and cells on the \n",
    "    # final Excel sheet.\n",
    "    merged_df = master_df.merge(\n",
    "            # Remove filename to avoid conflicts\n",
    "            analysis_df.drop(columns=[\"file_name\"]),\n",
    "            # Locate the appropriate in-sheet coordinates to store the extracted\n",
    "            # data\n",
    "            on=\"letter\",\n",
    "            how=\"left\"\n",
    "            )\n",
    "\n",
    "    # Save updated file\n",
    "    merged_df.to_excel(\"merged_narcissism.xlsx\", index=False)\n",
    "    \n",
    "    # Print a flag on screen to showcase termination of the merging operation.\n",
    "    print(\"Merging operation complete...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c701387-e654-484b-9979-9916796198d8",
   "metadata": {},
   "source": [
    "# narcissism.py\n",
    "This script calls all of the above functions at different and appropriate levels, in order to extract data from the master Excel spreadsheet, analyse each letter to shareholder independently, and merge the results into an Excel spreadsheet that is then going to be used in STATA.\n",
    "\n",
    "### Package Imports\n",
    "- `pandas`: package for reading Excel files and manipulating tabular data efficiently.\n",
    "- `Path from pathlib`: a path manipulation library to efficiently shift between sub-folders.\n",
    "- `setup`: the config.json loader script from before.\n",
    "- `re`: regular expression package for text cleaning.\n",
    "- `extractor`: custom extractor function defined above.\n",
    "- `analyst`: custom analyst function defined above.\n",
    "- `merger`: custom merger function defined above.\n",
    "- `setup`: custom setup function defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41af20d2-417a-4116-ada3-abca085ca871",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anenin/Documents/Projects/Python/venv/lib/python3.12/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell AC8204 is marked as a date but the serial value 2655400000.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id_firm         firm_name           coder    year             letter\n",
      "0        1  3D Systems Corp.  Marvin Hanisch  2005.0  letter_2005_1.txt\n",
      "1        1  3D Systems Corp.  Marvin Hanisch  2006.0  letter_2006_1.txt\n",
      "2        1  3D Systems Corp.  Marvin Hanisch  2007.0  letter_2007_1.txt\n",
      "3        1  3D Systems Corp.  Marvin Hanisch  2008.0  letter_2008_1.txt\n",
      "4        1  3D Systems Corp.  Marvin Hanisch  2009.0  letter_2009_1.txt\n",
      "Extraction complete...\n",
      "Analysis complete, see 'narcissism_analysis.xlsx' for results.\n",
      "Merging operation complete...\n"
     ]
    }
   ],
   "source": [
    "# narcissism.py\n",
    "\n",
    "# Packages & External Libraries\n",
    "import pandas as pd\n",
    "from extractor import *\n",
    "from analyst import *  \n",
    "from merger import *\n",
    "import re\n",
    "from pathlib import Path\n",
    "from setup import *\n",
    "\n",
    "def main():\n",
    "    '''\n",
    "    Main python program which performs analysis for narcissism on each individual letter to shareholders. In order to manipulate the program\n",
    "    into not performing the same (and completed operations) like extracting data from the Master spreadsheet, go to config.json and change\n",
    "    the according variables into \"true\" or \"false\". DONT USE CAPS, THIS IS NOT PYTHON.\n",
    "    '''\n",
    "    # Yield the config variables from config.json\n",
    "    extract_data, analyse_data, global_search, directory_search, merge_data = load_config_flags()\n",
    "\n",
    "    # if True, continue with the operation of extracting data from the main Excel spreadsheet.\n",
    "    if extract_data:\n",
    "        file_path = \"Firm_Data.xlsx\"        \n",
    "        extractor(file_path)\n",
    "\n",
    "    # if True continue with the operation of analyzing each letter to shareholders for singular/plural pronouns check, in order to \n",
    "    # generate the narcissism ratios for each CEO.\n",
    "    if analyse_data:\n",
    "        root_folder = Path(\"Letters\")   # Folder which contains the letters\n",
    "        results = []\n",
    "\n",
    "        # Main iteration loop, which scans each file and searches for the desired keywords\n",
    "        if global_search:\n",
    "            # Perform a global recursive search for all .txt files in the specified root folder. \n",
    "            # The goal here is to read all of the letters to shareholders and analyse them for narcissism ratios.\n",
    "            for txt_file in root_folder.rglob(\"*.txt\"):\n",
    "                result = analyst(txt_file)\n",
    "                if result:\n",
    "                    results.append(result)\n",
    "        elif directory_search:\n",
    "            # Perform a localised recursive search (only of parent folder's\n",
    "            # directories), with the same goal to read all .txt files and yield\n",
    "            # a narcissism ratio.\n",
    "            for subfolder in root_folder.iterdir():\n",
    "                if subfolder.is_dir():\n",
    "                    for txt_file in subfolder.glob(\"*.txt\"):\n",
    "                        result = analyst(txt_file)\n",
    "                        if result:\n",
    "                            results.append(result)\n",
    "        else:\n",
    "            # Return an indication that both search options are turned off.\n",
    "            # If you get this error check variables global_search & local_search at the config.json\n",
    "            print(\"No search method has been defined.\")\n",
    "\n",
    "        # Convert to a pandas DataFrame data structure for easier manipulation\n",
    "        df = pd.DataFrame(results)\n",
    "\n",
    "        # Save to Excel (and CSV)\n",
    "        df.to_excel(\"narcissism_analysis.xlsx\", index = False)\n",
    "        #df.to_csv(\"narcissism_analysis.csv\", index = False)   # uncomment this line if you also want a .csv file\n",
    "        \n",
    "        # Print a flag on screen to showcase termination of narcissism analysis operation.\n",
    "        print(\"Analysis complete, see 'narcissism_analysis.xlsx' for results.\")\n",
    "\n",
    "    # if True, continue with the operation of merging the extracted_data.xlsx and narcissism_analysis.xlsx together \n",
    "    # into a single Excel spreadsheet.\n",
    "    if merge_data:\n",
    "        merger()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b861f26-7838-4ae5-a5e5-6ad8c8d59478",
   "metadata": {},
   "source": [
    "# cata.py\n",
    "This script checks the business part of each 10k filling for each company in the specified root folder. Then, it imports the dictionary in which the clusters of digitalization words are stored, and computed the adt ratio of each 10k filling based on that dictionary. Finally, the results are stored into a separate Excel file, for easier manipulation later in STATA. \n",
    "\n",
    "### Package Imports\n",
    "- `pandas`: package for reading Excel files and manipulating tabular data efficiently.\n",
    "- `Path from pathlib`: a path manipulation library to efficiently shift between sub-folders.\n",
    "- `re`: regular expression package for text cleaning.\n",
    "- `os`: package for path manipulation between each folder.\n",
    "- `defaultdict from collections`: an enhanched dictionary data structure which is complementary to Python's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "392f69a2-5ddb-4c4e-ba47-210539920ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1596 text files to process.\n",
      "Output saved to 'cluster_keyword_hits.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# cata.py\n",
    "\n",
    "# Packages\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Import the dictionary in which the keywords and clusters are stored.\n",
    "df_dict = pd.read_excel(\"full_dictionary.xlsx\")\n",
    "# Initialize a set data structure for each category, i.e. for each keyword cluster.\n",
    "category_keywords = {}\n",
    "\n",
    "# Convert each column into a different category, i.e. map the keywords listed under each header to a cluster (a set) in Python.\n",
    "for col in df_dict.columns:\n",
    "    words = df_dict[col].dropna().str.lower().tolist()\n",
    "    # Replace hyphens with underscores for all words in the dictionary.\n",
    "    category_keywords[col] = [w.replace(\"-\", \"_\") for w in words]\n",
    "\n",
    "# Specify folder in which the .txt files are stored.\n",
    "root_folder = Path(\"Business\")\n",
    "\n",
    "# Recursively search each subfolber in the root folder for .txt files, and store their paths in a list.\n",
    "text_files = list(root_folder.rglob(\"*.txt\"))\n",
    "\n",
    "# Print a flag on screen to showcase the number of files to be analysed. This serves as an indication for the user.\n",
    "print(f\"Found {len(text_files)} text files to process.\")\n",
    "\n",
    "# Initialize an output list\n",
    "output = []\n",
    "\n",
    "# Iterate over each relevant path and filename (STUDENT/business_DATA_ID.txt) and analyse them for digitalization ratios.\n",
    "# Perform each operation in a try-except block, in order to avoid premature termination due to an unexpected error.\n",
    "for filepath in text_files:\n",
    "    try:\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            # Read the file for its contents.\n",
    "            raw_text = f.read()\n",
    "            # Convert the body of the file to lowercase\n",
    "            lowercase_text = raw_text.lower()\n",
    "            # Normalize text in the file, i.e. convert whitespaces and hyphens to underscores. \n",
    "            normalized_text = re.sub(r\"[\\s\\-]+\", \"_\", lowercase_text)\n",
    "        \n",
    "            # Count the total words in the document\n",
    "            word_count = len(re.findall(r\"\\b\\w+\\b\", raw_text))\n",
    "            # Initialize the total_hits and category_hits variables, both columns in the final Excel file.\n",
    "            total_hits = 0\n",
    "            category_hits = {}\n",
    "\n",
    "        # Iterate over each category and keyword to find matches in the text.\n",
    "        for category, keywords in category_keywords.items():\n",
    "            matches = [kw for kw in keywords if kw in normalized_text]\n",
    "            category_hits[category] = matches\n",
    "            total_hits += len(matches)\n",
    "\n",
    "        # Build the output Excel document in an appropriate format for later use in STATA.\n",
    "        row = {\"Filename\": filepath.name}\n",
    "        # Iterate over each category.\n",
    "        for category in category_keywords:\n",
    "            # Create a column for each word cluster in the dictionary.\n",
    "            row[category] = len(category_hits.get(category, []))\n",
    "        # Add total matches, word count and adt ratio to the final Excel file.\n",
    "        row[\"Total_Matches\"] = total_hits\n",
    "        row[\"Word_Count\"] = word_count\n",
    "        row[\"Match_Ratio\"] = round(total_hits / word_count, 4) if word_count > 0 else 0\n",
    "        output.append(row)\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Handle any and all errors.\n",
    "        print(f\"Error reading {filepath}: {e}\")\n",
    "\n",
    "# Convert the DataFrame data structure into an .xlsx document.\n",
    "df_out = pd.DataFrame(output)\n",
    "df_out.to_excel(\"cluster_keyword_hits.xlsx\", index=False)\n",
    "\n",
    "# Print a flag on screen to showcase termination of file.\n",
    "print(\"Output saved to 'cluster_keyword_hits.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b05e986-38cb-4277-b127-36b88de7bc8e",
   "metadata": {},
   "source": [
    "# How to use this code?\n",
    "- Take each cell of this jupyter notebook, and run it in order to initialize each script in Python's memory.\n",
    "- In case you don't have some of the libraries, then make sure to download them.\n",
    "- Make sure to change the **PATHS** to the initial dataset, the dictionary etc. **INSIDE** each relevant Python script. Follow Python's traceback errors.\n",
    "- Run `narcissism.py` and `cata.py`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
